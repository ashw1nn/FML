{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERAL EXPLANATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.  First, import the necessary libraries \n",
    "1. Numpy\n",
    "2. Pandas\n",
    "3. Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Run main.py to get the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Before loading the dataset, I have directly added column headers x1, x2 and y on the csv files for better understanding and viewing (in both train and test dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code starts by importing all the necessary libraries required as stated above. Then all the required functions have been defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part of code starts by loading the datasets \n",
    "I have added bias terms in both the training data and test data as shown below(Create a seperate ones column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"FMLA1Q1Data_train.csv\")\n",
    "test_data = pd.read_csv(\"FMLA1Q1Data_test.csv\")\n",
    "train_data[\"x0\"] = 1\n",
    "test_data[\"x0\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I have splitted the features and labels as X and y  for both train and test datasets\n",
    "In the following subdivision, explanation for each part of the question is given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for linear regression is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    y = w0 + w1 * x1 + w2 * x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding solution for w in matrix form is \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    w* = (X * X.T) * X * Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X.T = transpose of X matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_solution(X, y):\n",
    "    w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    w = np.array(w)\n",
    "    w.reshape(-1,1)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the analytical solution function used as per the formula. Please note that as I have taken X as a n*d matrix, So corresponding changes have been made (I replaced X by X.T)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE EXPLANATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = np.zeros(X_train.shape[1])\n",
    "epochs = 100\n",
    "error_function = np.zeros(epochs)\n",
    "\n",
    "learning_rates = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "best_l = 0\n",
    "weight_error = np.inf\n",
    "for l in learning_rates :\n",
    "    w = gradient_descent(X_train, y_train, w_init, l, epochs, w_ml, error_function)\n",
    "    if mse(w, w_ml) < weight_error :\n",
    "        weight_error = mse(w, w_ml)\n",
    "        best_l = l\n",
    "\n",
    "w = gradient_descent(X_train, y_train, w_init, best_l, epochs, w_ml, error_function)\n",
    "\n",
    "plt.title(\"Gradient Descent Error Plot\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error b/w analytical and predicted weights\")\n",
    "plt.plot(error_function)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I have initialised weights as zero. Then , I have set epochs to 100 (I found out this is enough no. of iterations for convergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have also created a error function variable to store the squared error between the weights of analytical and gradient descent solution. This will be used in plotting the graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out the best learning rate for the gradient descent, I have taken a set of values in different order as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, For each of these learning rate, I have done the gradient descent algorithm. I took the best learning rate as the one which provides the most similar solution to the analytical solution. The solution which has the least error w.r.t to analytical is the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I have plotted the graph as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent (X, y, w, learning_rate, epochs, w_ml, error_function):\n",
    "\n",
    "    for i in range(epochs):\n",
    "        gradient = (2 * (X.T @ ((X @ w) - y)))\n",
    "        w = w - learning_rate * gradient\n",
    "        error_function[i] = np.sum((w - w_ml) ** 2)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the gradient descent function I have used. The gradient and update function is calculated as per lectures taught in the class. I have added the error function update for plotting the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSERVATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a learning rate of = 0.0001, the gradient descent solution converges to the analytical solutions. Lower learning rates will also converge but will take more iterations so this is ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLANATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = np.zeros(X_train.shape[1])\n",
    "epochs = 20\n",
    "error_function= np.zeros(epochs)\n",
    "\n",
    "w_stochastic = stochastic_gradient_descent(train_data, w_init, 0.0001, epochs, w_ml, error_function)\n",
    "\n",
    "plt.title(\"Stochiastic Gradient Descent Error Plot\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error b/w analytical and predicted weights\")\n",
    "plt.plot(error_function)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar code as the previous gradient descent one. We'll move on to the explanation of the function which is commmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(data, w, learning_rate, epochs, w_ml, error_function):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Shuffle the training data for each epoch\n",
    "        data.sample(frac=1).reset_index(inplace=True, drop=True)\n",
    "    \n",
    "        no_batches = 10 # Each batch has 100 data points \n",
    "        \n",
    "        # Iterate over each batch\n",
    "        for i in range(0, no_batches):\n",
    "\n",
    "            st_idx = i*int(len(data)/no_batches)\n",
    "            end_idx = (i+1)*int(len(data)/no_batches)\n",
    "\n",
    "            # Separate the features (X_rand) and target (y_rand)\n",
    "            X_rand = data.iloc[st_idx:end_idx].drop(\"y\", axis=1)\n",
    "            y_rand = data.iloc[st_idx:end_idx][\"y\"]\n",
    "            \n",
    "            # Calculate the gradient\n",
    "            gradient = 2 * (X_rand.T @ ((X_rand @ w) - y_rand))\n",
    "            \n",
    "            # Update the weights\n",
    "            w = w - learning_rate * gradient\n",
    "            \n",
    "            # Calculate and store the error for the current epoch\n",
    "            \n",
    "        error_function[epoch] = np.sum((w - w_ml) ** 2)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSERVATIONS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observarion is that the stochastic method converges faster than the normal gradient descent. In my case, The number of iterations it took to converge is approximaately the same but it might be same for a very large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the ridge regression function is similar to gradient descent with a small addition of regularizer. This is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regresssion(X, y, w, learning_rate, epochs, lamda):\n",
    "    for i in range(epochs):\n",
    "        gradient = (2 * (X.T @ ((X @ w) - y)) + 2 * lamda* w) / len(y)\n",
    "        w = w - learning_rate * gradient\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cross valiation, I have splitted the train data into train and validation(80-20 split), then found the best lambda value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "best_lamda = 0\n",
    "\n",
    "lambda_error = np.inf\n",
    "y_val_pred_ml = X_val_split @ w_ml\n",
    "for lam in lamda :\n",
    "    w_ridge = ridge_regresssion(X_train_split, y_train_split, w_init, 0.0001, epochs, lam)\n",
    "    y_val_pred_ridge = X_val_split @ w_ridge\n",
    "    if mse(y_val_pred_ml, y_val_pred_ridge) < lambda_error:\n",
    "        lambda_error = mse(y_val_pred_ml, y_val_pred_ridge)\n",
    "        best_lamda = lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have found the error between predicted y values from analytical and ridge solutions. The best lambda is the one which produces the least error here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I have printed the Ridge and Analytical error. I found out the ridge error is more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have did gaussian kernel regression as it works even in non linear data set . And when I tried this here, I got a very low test error of 0.98. So, this kernel regression is better than other types of regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guassian_kernel(x, xi, mu):\n",
    "    z = np.linalg.norm(xi- x)/mu\n",
    "    return (np.exp((-z**2)/2))/(np.sqrt(2*np.pi))\n",
    "\n",
    "def kernel_regression(X_train, X_test, mu):\n",
    "    weights = []\n",
    "    for i, xi in X_test.iterrows():\n",
    "        kernels = [guassian_kernel(x, xi, mu) for idx, x in X_train.iterrows()]\n",
    "        sim_score = [X_train.shape[0]*kernel/np.sum(kernels) for kernel in kernels]\n",
    "        weights.append(sim_score)\n",
    "    weights = np.array(weights)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I have defined the gaussian kernel function (basic) and then defined the kernel regression function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
